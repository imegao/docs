
## TiDB部署过程
1. 中控机上安装系统依赖包(root身份）
``` 
    #yum -y install epel-release git curl sshpass
    #yum -y install python-pip
```
 1. 创建tidb用户，并配置免密
``` 
    #useradd -m -d /home/tidb tidb
    #passwd tidb
    #visudo
```
     添加
```
    tidb ALL=(ALL) NOPASSWD: ALL
```
    修改每一个服务器名称:
```
    #vi /etc/hostname
```
    修改IP地址映射,并修改文件/etc/hosts
```
    #vi /etc/hosts
```
    在里面加入(改成部署的IP与hostname映射):
```
    192.168.218.134 node1
    192.168.218.135 node2
    192.168.218.136 node3
```
    重启后切换到tidb用户
```
    #su tidb
```
    生成ssh密钥，将密码保留为空：
```
    $ssh-keygen -t rsa
```
按提示直接回车，生成的ssh私钥文件为/home/tidb/.ssh/id_rsa
 2. 在中控机下载TiDB-Ansible（使用tidb用户）
```
    $git clone -b release-2.1 https://github.com/pingcap/tidb-ansible.git
```
 3. 按照下面命令安装Ansible及其依赖
```
    $cd /home/tidb/tidb-ansible
    #sudo pip install -r ./requirements.txt
    #ansible --version 
```
 4. 配置部署机器ssh互信与sudo规则
```  
    #cd /home/tidb/tidb-ansible
    $vi hosts.ini
```
```
    [servers]
    172.16.10.1
    172.16.10.2
    172.16.10.3
    172.16.10.4
    172.16.10.5
    172.16.10.6
    [all:vars]
    username = tidb
    ntp_server = pool.ntp.org
```
配置hosts.ini文件将目标机器IP添加到[servers]区块下
   
执行以下命令按提示输入部署目标机器 root 用户密码。该步骤将在部署目标机器上创建 tidb 用户，并配置 sudo 规则，配置中控机与部署目标机器之间的 ssh 互信。
```
    $ansible-playbook -i hosts.ini create_users.yml -u root -k
```
 5. 部署目标机器安装NTP服务（注意如果集群不能连接外网需要修改ntp server地址）
```
    $cd /home/tidb/tidb-ansible
    $ansible-playbook -i hosts.ini deploy_ntp.yml -u tidb -b
```
 6. 在部署目标机器上配置 CPUfreq 调节器模式，目的是为了让CPU发挥最大性能，将CPUfreq     调节器模式设置为 performance 模式
 通过
```
    #cpupower frequency-info --governors
```
    查看系统支持的调节器模式，如果返回 “Not Available”，表示当前系统不支持配置 CPUfreq，跳过该步骤即可。
 通过
``` 
    #cpupower frequency-info --policy
```
查看当前CPUfreq 调节器模式
通过
```
    #cpupower frequency-set --governor performance
```
修改为performance模式
 7. 在所有部署目标机器上添加数据盘ext4文件系统挂载参数,部署目标机器数据盘请格式化成      ext4 文件系统，挂载时请添加 nodelalloc 和 noatime 挂载参数。nodelalloc 是必选     参数，否则 Ansible 安装时检测无法通过，noatime 是可选建议参数。
    如果你的数据盘已经格式化成 ext4 并挂载，可先执行 umount 命令卸载，从编辑 /etc/fstab 文件步骤开始执行，添加挂载参数重新挂载即可。
```
    #umount /dev/{nvme0n1}
```
查看数据盘
```
    #fdisk -l    
```
创建分区表
```
    #parted -s -a optimal /dev/sda1 mklabel gpt -- mkpart primary ext4 1 -1
```
格式化文件系统
```
    #mkfs.ext4 /dev/{nvme0n1}      
```
查看数据盘分区UUID，copy下{nvme0n1}的UUID
```
    #lsblk -f     
    #vi /etc/fstab
```
添加nodelalloc挂载参数{例如：UUID=c51eb23b-195c-4061-92a9-3fad812cc12f /data1 ext4 defaults,nodelalloc,noatime 0 2}
```
    #mkdir /data1
    #mount -a
```
挂载数据盘到data1目录下
```
    #mount -t ext4  
    /dev/nvme0n1 on /data1 type ext4 (rw,noatime,nodelalloc,data=ordered)
```
如果文件系统为ext4并且挂载参数中包含nodelalloc表示生效
8. 分配机器资源并修改inventory.ini文件，在配置文件对应位置添加ip（tidb+PD+PumP+        tikv等设置）
    例如TiDB集群有6台机器，2个TiDB节点、3个PD节点、3个TiKV节点第一台TiDB机器同时用作监控机，配置如下：
```
    [tidb_servers]
    172.16.10.1
    172.16.10.2

    [pd_servers]
    172.16.10.1
    172.16.10.2
    172.16.10.3

    [tikv_servers]
    172.16.10.4
    172.16.10.5
    172.16.10.6

    [monitoring_servers]
    172.16.10.1

    [grafana_servers]
    172.16.10.1

    [monitored_servers]
    172.16.10.1
    172.16.10.2
    172.16.10.3
    172.16.10.4
    172.16.10.5
    172.16.10.6
```       
9.  inventory.ini 变量调整，主要是修改deploy_dir  如/data1/deploy ，data1为数据盘     挂载目录。
    其他参数设置请参考[PingCAP University文档](https://pingcap.com/docs-cn/dev/how-to/deploy/orchestrated/ansible/)
10.  部署tidb
确认tidb-ansible/inventory.ini 文件中 ansible_user = tidb，不要设置成 root 用户，tidb-ansible 限制了服务以普通用户运行。
 验证ssh互信,返回 tidb则表示成功.
```
    $ansible -i inventory.ini all -m shell -a 'whoami'
```
验证tidb用户的sudo免密配置成功,返回root则表示成功.
```
    $ansible -i inventory.ini all -m shell -a 'whoami' -b
```
下载TiDB binary 到中控机
```
    $ansible-playbook local_prepare.yml
```
初始化系统环境，修改内核参数
```
    $ansible-playbook bootstrap.yml
```
部署集群
```
    $ansible-playbook deploy.yml
```
【注意】：
    **确保目标机器上添加数据盘**<u>*ex4* </u>文件系统挂载参数,否则会Ansible FAILED。
    如产生"swap is on，for best performance，turn swap off"的error请关闭分区。
```
    #swapoff -a 
```
    启动TiDB集群
```
    $ansible-playbook start.yml
```
【注意】：
    如产生"...could not find the requested service tidb-4000..."可能是tidb服务未部署成功，重新执行
```
    $ansible -playbook deploy.yml -t tidb
```
    再次启动，成功后使用mysql客户端进行测试
```
    $mysql -u root -h 172.16.10.1 -P 4000
```
具体参考参照：
    [TiDB官方Ansible部署文档](https://www.pingcap.com/docs-cn/op-guide/ansible-deployment/)
# Ceph部署过程
## 安装Ceph-deploy
1. 配置下载源
```
    #sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
``` 
```
    #cat << EOM > /etc/yum.repos.d/ceph.repo  
    [ceph-noarch]   
    name=Ceph noarch packages  
    baseurl=https://download.ceph.com/rpmluminous/el7/noarch  
    enabled=1  
    gpgcheck=1  
    type=rpm-md  
    gpgkey=https://download.ceph.com/keys/release.asc  
    EOM
```
更新yum存储库
```
    #sudo yum update
```
安装最新Ceph-deploy：
```
    #sudo yum install ceph-deploy
```
配置源：
```
    #yum install centos-release-ceph-luminous.noarch
```
此步不执行可能会产生"ceph-deploy create node error：ensuring /etc/yum.repo/ceph.repo cotains a high priority"
 2. （如果TiDB部署ntp服务此步骤可跳过）每个节点创建安装NTP确保每个Ceph节点时间同步：
```
    #sudo yum install ntp ntpdate ntp-doc
```
（无外网情况下）在子服务器的计划任务/etc/crontab文件下配置与Ceph管理节点时钟同步
```
    0-59/5 * * * root /usr/sbin/ntpdate 192.168.x.x
``` 
（有外网情况下）设置管理节点与外网时钟同步编辑管理节点/etc/crontab文件加入
```
    0 1 * * * root /usr/sbin/ntpdate ntp.api.bz
```
 3. （TiDB部署修改过可忽略）修改每个服务器名为部署的名称并重启
```
    #vi /etc/hostname
```
将所有节点修改 
```
    #vi /etc/hosts
```
添加 "[ip地址] [节点名称]"例如 192.18.222.129  master
 4. ssh互信（TiDB做过此步骤可忽略）
在每个Ceph节点创建一个新用户：
```
    #sudo useradd -d /home/{username} -m {username} 
```
把{username}改为自己需要的用户名
设置密码
```
    #sudo passwd {username}
```
对于添加到每个Ceph节点的新用户，确保有sudo权限
``` 
    #echo "{username} ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/{username}
    #sudo chmod 0440 /etc/sudoers.d/{username} 
```
生成ssh密钥将密码保留为空：
```
    #ssh-keygen
```
并将公钥分发给每个Ceph节点
```
    #ssh-copy-id {username}@node1
    #ssh-copy-id {username}@node2
    #ssh-copy-id {username}@node3
```
修改管理节点的~/.ssh/config文件，ceph-deploy会用创建的用户登录ceph节点，而不用每次都指定(根据自己的节点更换相应的信息)
```
    Host node1
        Hostname node1
        User user1
    Host node2
        Hostname node2
        User user2
    Host node3
        Hostname node3
        User user3
```
 5. 打开必须的端口：ceph-mon默认以6789通信  osd端口范围在6800:7300
    在监控节点:
```
    #sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent
```
在osd上：
```
    #sudo firewall-cmd --zone=public --add-service=ceph --permanent
```
【注意】：使用--permanent配置firewalld后，可以立即生效，不用重启
```
    #sudo firewall-cmd --reload
```
为iptables添加ceph-mon端口 : 
```
    #sudo iptables -A INPUT -i ens33 -p tcp -s 192.168.218.134/255.255.255.0 --dport 6789 -j ACCEPT
```
【注意】：iface 是服务器连接网线的网口，通过ethtool命令查看网口是否连接 ip-address 是服务器网络地址，netmask是网络掩码）
    配置完成后时生效：
```
    #/sbin/service iptables save 
```
       
6. 临时关闭selinux
    查看selinux状态
```
    #getenforce Enforcing
```
    关闭selinux
```
    #setenforce 0
    #getenforce Permissive
```
## 使用Ceph-deploy安装Ceph
1. 创建ceph集群
```
    #ceph-deploy new node1 node2 node3
```
（*注：需要把各ceph节点的hostname改为对应的名字，如 ` hostname node1`）
```
    #ceph-deploy install --no-adjust-repos node1 node2 node3 --release nautilus
```
（*注：单节点执行失败可以在此节点单独yum install ceph）
修改ceph.conf文件
```
    #vim ceph.conf
    public_network = */24
```
*为本机ip，24为子网掩码数部署。
初始监视器并收集密钥：
```
    #ceph-deploy mon create-initial
```
使用ceph-deploy配置文件和管理密钥复制到ceph各节点：
```
    #ceph-deploy admin node1 node2 node3
```
部署管理器守护程序：
```
    #ceph-deploy mgr create node1
```
添加osd，先保证每个节点都有一个未使用过的磁盘
```       
    #ceph-deploy osd create --data /dev/sdb node2
    #ceph-deploy osd create --data /dev/sdb node3
```
 2. ceph-deploy删除节点相关命令（如果部署失败可使用此命令进行节点删除）
```
    #ceph-deploy purge {ceph-node} [{ceph-node}]
    #ceph-deploy purgedata {ceph-node} [{ceph-node}]
    #ceph-deploy forgetkeys 
    #rm ceph.*
```
 2. 查看完整的集群状态：
```
    #ceph -s
```
 3. 将对象存储到ceph集群中，ceph需要设置对象名称
    创建pool命令pg-num与pgp-num 最好一致
```
    #ceph osd pool create {pool-name} {pg-num} [{pgp-num}]
    #ceph osd pool create  tiger  128 128
    #ceph osd pool create  rabbit  128 128
```
此处pgp设置参考pg与pgp设置算法：
计算公式：(单个osd上pg数，一般为100) x (osd数量) x (该pool数据量百分比) / (副本数)。所得结果向上取2的整数倍[pg与pgp设置参考网站](https://ceph.com/pgcalc/)
部署参考 : [ceph官网](http://docs.ceph.com/docs/master/start/quick-ceph-deploy/)

## 部署Caddy
1. 下载caddy源码[yig-front-caddy](https://github.com/journeymidnight/yig-front-caddy/)
```
    #git clone https://github.com/journeymidnight/yig-front-caddy.git
```
2. 将证书文件、prometheus配置文件、mime.type导入yig-front-caddy/caddy目录下
3. 新建caddyfile，参考[caddy/docs](https://caddyserver.com/docs/)
    例如：
```
    *.s3.test.com:443 https://s3.test.com {
    import mime.types
    import prometheus

    tls leaf.pem leaf.key

    log / access.log "{combined}" {
        rotate_size 50  # Rotate after 50 MB
        rotate_age  90  # Keep rotated files for 90 days
        rotate_keep 20  # Keep at most 20 log files
        rotate_compress # Compress rotated log files in gzip format
    }

    errors error.log {
        rotate_size 50  # Rotate after 50 MB
        rotate_age  90  # Keep rotated files for 90 days
        rotate_keep 20  # Keep at most 20 log files
        rotate_compress # Compress rotated log files in gzip format
    }

    rewrite {
        if {?x-oss-process} not ""
        to /cropimage/{host}{path}
    }

    rewrite {
	    if {?x-oss-info} not ""
            to /ossadmin/{?x-oss-info}
    }
    proxy / http://127.0.0.1:8080 {
	    header_upstream Host {host}
        header_upstream X-Real-IP {remote}
        header_upstream X-Forwarded-For {remote}
        header_upstream X-Forwarded-Proto {scheme}
	    health_check_timeout 10s
	}

    proxy /cropimage http://127.0.0.1:9001 {
        without /cropimage
        transparent
    }

    proxy /ossadmin http://127.0.0.1:9000 {
        without /ossadmin
        transparent
    }

	mime {
	    Query x-oss-download application/octet-stream
	}
    }

    http://s3.test.com *.s3.test.com:80 {
    import mime.types
    import prometheus


    log / access.log "{combined}" {
        rotate_size 50  # Rotate after 50 MB
        rotate_age  90  # Keep rotated files for 90 days
        rotate_keep 20  # Keep at most 20 log files
        rotate_compress # Compress rotated log files in gzip format
    }

    errors error.log {
        rotate_size 50  # Rotate after 50 MB
        rotate_age  90  # Keep rotated files for 90 days
        rotate_keep 20  # Keep at most 20 log files
        rotate_compress # Compress rotated log files in gzip format
    }

    rewrite {
        if {?x-oss-process} not ""
        to /cropimage/{host}{path}
    }

    rewrite {
        if {?x-oss-info} not ""
        to /ossadmin/{?x-oss-info}
    }
    proxy / http://127.0.0.1:8080 {
        header_upstream Host {host}
        header_upstream X-Real-IP {remote}
        header_upstream X-Forwarded-For {remote}
        header_upstream X-Forwarded-Proto {scheme}
    }
    proxy /cropimage http://127.0.0.1:9001 {
        without /cropimage
        transparent
    }

    proxy /ossadmin http://127.0.0.1:9000 {
        without /ossadmin
        transparent
    }

    mime {
    }
}
```
4. 在caddy文件夹下buildcaddy源码
```
    #go version
```
如果没有go环境安装golang
```
    #yum install golang
```
```
    #go run build.go
```
如果产生go get安装一些第三方库的网络问题"golang.org/x/sys/unix"请手动下载相应的package
```
    #mkdir -p #GOPATH/src/golang.org/x/
    #cd !#
    #git clone https://github.com/golang/net.git
    #git clone https://github.com/golang/sys.git
    #git clone https://github.com/golang/tools.git
```
5. 在caddy文件夹下启动caddy
```
    #./caddy
```
 ## Prometheus安装部署
 1. 关闭tidb自带的promethus，进入/deploy/scripts/ 执行
```
    #sh stop_prometheus.sh
```
 2. 下载prometheus安装包https://github.com/prometheus/prometheus/releases/tag/v1.6.2
 ```
 #wget https://github.com/prometheus/prometheus/releases/download/v2.10.0/prometheus-2.10.0.linux-amd64.tar.gz
 ```
 3. 解压安装包
```
    #tar -xvf prometheus-1.6.2.linux-amd64.tar.gz
```
4. 修改配置文件
```
    - job_name: 'nginx'
        static_configs:
        - targets: ['{ip}:4040']
        - job_name: 'usage'
        static_configs:
    - targets: ['{ip}:9000']
```
注释掉：
```
    - job_name: 'prometheus'
```
5. 后台启动Prometheus
```
    #nohup ./prometheus -config.file=prometheus.yml &
```
## redis部署
1. 下载redis压缩包，解压并进入文件夹
```    
    #wget http://download.redis.io/releases/redis-5.0.4.tar.gz tar xzf redis-5.0.4.tar.gz 
    #cd redis-5.0.4
    #make
```
    进入src目录下
```
    #cd src
    #make install PREFIX=/usr/local/redis
```
2. 移动配置文件到安装目录
```
    #cd ../
    #mkdir /usr/local/redis/etc
    #mv redis.conf /usr/local/redis/etc
```
3. 后台启动redis
    
```
    #vi /etc/redis.conf
```
修改deamonize属性为yes   
```
    #redis-server ~/redis/redis.conf
```

4. 开启redis  
```
    #/usr/local/redis/bin/redis-server  /usr/local/redis/etc/redis.conf 
```
执行
```
    #src/redis-cli
```
进入设置，使用
```
    #config set requirepass [password]
```
修改redis的密码（与yig设置的redis密码一致）
## Yig部署流程
### 使用源码安装Yig
1. 下载yig源码，[yig代码](https://github.com/journeymidnight/yig/)
```
    #git clone https://github.com/journeymidnight/yig.git
```
2. build
```
    cd yig
    go get ./...
    go build
```
3. 修改配置文件，进入yig/integrate/yigconf,修改yig.toml中的redis与tidb的配置
4. 安装配置dnsmasq与dns服务器
    安装配置dnsmasq：
```
    #yum install dnsmasq -y 
    #sudo vim /etc/dnsmasq.conf
```
添加address=/.s3.test.com/127.0.0.1
重启dnsmasq
```
    #systemctl restart dnsmasq
```
配置DNS服务器
```
    #vi /etc/resolv.conf
```
添加
```
    #nameserver 127.0.0.1
```
1. 进入yig源码目录build源码
```
    #go build
```
如果出现版本不兼容问题，安装docker容器
```
    #yum insall docker
```
然后在yig目录下执行
```
    #make build 
```
5. 安装s3cmd，生成./s3cfg文件，修改host_base与host_bucket等配置（与dnsmasq中的        s3.test.com对应）
```
    #yum install s3cmd
```
s3cfg文件内容如下：
```    
    [default]
    access_key = hehehehe
    secret_key = hehehehe
    default_mime_type = binary/octet-stream
    enable_multipart = True
    encoding = UTF-8
    encrypt = False
    use_https = False
    host_base = s3.test.com:8080
    host_bucket = %(bucket)s.s3.test.com:8080
    multipart_chunk_size_mb = 128
```
6. 启动Yig前使用ceph创建两个pool
    连接mysql,ip为你部署mysql的ip地址
```
    #mysql -u root -h 172.16.10.1 -P 4000
```
    使用mysql创建数据库，并把/yig/integrate/yig.sql导入到数据库中
```
    #create database yig
    #source ../yig/integrate/yig.sql
```    
7. 在yig目录下启动yig
```
    #./yig
```
 ### 使用rpm安装Yig
 1. 下载Yig最新rpm包
```
    #wget https://github.com/journeymidnight/yig/releases/download/v1.2.3/yig-1.1-612.bde24b7git.el7.x86_64.rpm
```
    如果没有wget 工具使用 
```
    #curl -O https://github.com/journeymidnight/yig/releases/download/v1.2.3/yig-1.1-612.bde24b7git.el7.x86_64.rpm
``` 
2. 安装rpm包
```
    #rpm -ivh yig-1.1-612.bde24b7git.el7.x86_64.rpm
``` 
1. 启动Yig前使用ceph创建两个pool
    连接mysql,ip为你部署mysql的ip地址
```
    #mysql -u root -h 172.16.10.1 -P 4000
```
    使用mysql创建数据库，并把/yig/integrate/yig.sql导入到数据库中
```
    #create database yig
    #source ../yig/integrate/yig.sql
```    